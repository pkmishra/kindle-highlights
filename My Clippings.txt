Designing Data-Intensive Applications-9781491903063 (Martin Kleppmann)
- Your Highlight on Location 8173-8174 | Added on Sunday, October 15, 2023 9:45:03 PM

The best way of building fault-tolerant systems is to find some general-purpose abstractions with useful guarantees, implement them once, and then let applications rely on those guarantees.
==========
﻿Designing Data-Intensive Applications-9781491903063 (Martin Kleppmann)
- Your Highlight on Location 8185-8186 | Added on Monday, October 16, 2023 11:34:52 PM

If two nodes both believe that they are the leader, that
==========
﻿Designing Data-Intensive Applications-9781491903063 (Martin Kleppmann)
- Your Highlight on Location 8186-8186 | Added on Monday, October 16, 2023 11:35:00 PM

situation is called split brain, and it often leads to data loss.
==========
﻿Designing Data-Intensive Applications-9781491903063 (Martin Kleppmann)
- Your Highlight on Location 8222-8224 | Added on Monday, October 16, 2023 11:45:25 PM

transaction isolation is primarily about avoiding race conditions due to concurrently executing transactions, whereas distributed consistency is mostly about coordinating the state of replicas in the face of delays and faults.
==========
﻿Designing Data-Intensive Applications-9781491903063 (Martin Kleppmann)
- Your Highlight on Location 8241-8241 | Added on Monday, October 16, 2023 11:47:21 PM

make a system appear as if there were only one copy of the data, and all operations on it are atomic.
==========
Designing Data-Intensive Applications-9781491903063 (Martin Kleppmann)
- Your Highlight on Location 8258-8258 | Added on Saturday, October 28, 2023 10:52:48 PM

Linearizable?
==========
Quick Start Guide to Large Language Models - Sinan Ozdemir (Sinan Ozdemir)
- Your Highlight on page 38 | Location 500-502 | Added on Monday, October 30, 2023 11:29:07 PM

Language modeling is a subfield of NLP that involves the creation of statistical/deep learning models for predicting the likelihood of a sequence of tokens in a specified vocabulary (a limited and known set of tokens). There are generally two kinds of language modeling tasks out
==========
Quick Start Guide to Large Language Models - Sinan Ozdemir (Sinan Ozdemir)
- Your Highlight on page 38 | Location 500-503 | Added on Monday, October 30, 2023 11:29:48 PM

Language modeling is a subfield of NLP that involves the creation of statistical/deep learning models for predicting the likelihood of a sequence of tokens in a specified vocabulary (a limited and known set of tokens). There are generally two kinds of language modeling tasks out there: autoencoding tasks and autoregressive tasks
==========
Quick Start Guide to Large Language Models - Sinan Ozdemir (Sinan Ozdemir)
- Your Highlight on page 40 | Location 525-526 | Added on Friday, November 24, 2023 4:26:28 PM

LLMs are language models may be either autoregressive, autoencoding, or a combination of the two.
==========
Quick Start Guide to Large Language Models - Sinan Ozdemir (Sinan Ozdemir)
- Your Highlight on page 40 | Location 525-529 | Added on Friday, November 24, 2023 4:27:02 PM

LLMs are language models may be either autoregressive, autoencoding, or a combination of the two. Modern LLMs are usually based on the Transformer architecture (which we will use in this book), but can also be based on another architecture. The defining features of LLMs are their large size and large training datasets, which enable them to perform complex language tasks, such as text generation and classification, with high accuracy and with little to no fine-tuning.
==========
Quick Start Guide to Large Language Models - Sinan Ozdemir (Sinan Ozdemir)
- Your Highlight on page 51 | Location 669-670 | Added on Tuesday, November 28, 2023 11:33:31 PM

Transfer learning is a technique used in machine learning to leverage the knowledge gained from one task to improve performance on another related task.
==========
Quick Start Guide to Large Language Models - Sinan Ozdemir (Sinan Ozdemir)
- Your Highlight on page 54 | Location 702-703 | Added on Tuesday, November 28, 2023 11:35:09 PM

backpropagation—a mechanism to update model parameters to minimize errors.
==========
Quick Start Guide to Large Language Models - Sinan Ozdemir (Sinan Ozdemir)
- Your Highlight on page 54 | Location 710-712 | Added on Tuesday, November 28, 2023 11:36:00 PM

Attention is a mechanism used in deep learning models (not just Transformers) that assigns different weights to different parts of the input, allowing the model to prioritize and emphasize the most important information while performing tasks like translation or summarization.
==========
Quick Start Guide to Large Language Models - Sinan Ozdemir (Sinan Ozdemir)
- Your Highlight on page 59 | Location 758-759 | Added on Tuesday, November 28, 2023 11:39:10 PM

In NLP, embeddings are used to represent the words, phrases, or tokens in a way that captures their semantic meaning and relationships with other words.
==========
Quick Start Guide to Large Language Models - Sinan Ozdemir (Sinan Ozdemir)
- Your Highlight on page 60 | Location 780-780 | Added on Tuesday, November 28, 2023 11:41:37 PM

Tokens make up an LLM’s static vocabulary and don’t always represent entire words.
==========
Quick Start Guide to Large Language Models - Sinan Ozdemir (Sinan Ozdemir)
- Your Highlight on page 61 | Location 789-790 | Added on Tuesday, November 28, 2023 11:42:49 PM

Tokenization can also involve preprocessing steps like casing, which refers to the capitalization of the tokens.
==========
Deep Learning for Coders with fastai and P - Sylvain Gugger (Sylvain Gugger)
- Your Highlight on Location 359-360 | Added on Friday, December 15, 2023 6:59:16 PM

They showed that a single layer of these devices was unable to learn some simple but critical mathematical functions (such as
==========
Deep Learning for Coders with fastai and P - Sylvain Gugger (Sylvain Gugger)
- Your Highlight on Location 359-360 | Added on Friday, December 15, 2023 6:59:22 PM

They showed that a single layer of these devices was unable to learn some simple but critical mathematical functions (such as XOR)
==========
Deep Learning for Coders with fastai and P - Sylvain Gugger (Sylvain Gugger)
- Your Highlight on Location 360-361 | Added on Friday, December 15, 2023 6:59:29 PM

using multiple layers of the devices would allow these limitations to be addressed.
==========
Deep Learning for Coders with fastai and P - Sylvain Gugger (Sylvain Gugger)
- Your Highlight on Location 356-358 | Added on Friday, December 15, 2023 6:59:45 PM

An MIT professor named Marvin Minsky (who was a grade behind Rosenblatt at the same high school!), along with Seymour Papert, wrote a book called Perceptrons
==========
Deep Learning for Coders with fastai and P - Sylvain Gugger (Sylvain Gugger)
- Your Highlight on Location 363-373 | Added on Friday, December 15, 2023 7:01:34 PM

the most pivotal work in neural networks in the last 50 years was the multi-volume Parallel Distributed Processing (PDP) by David Rumelhart, James McClelland, and the PDP Research Group, released in 1986 by MIT Press. Chapter 1 lays out a similar hope to that shown by Rosenblatt: People are smarter than today’s computers because the brain employs a basic computational architecture that is more suited to deal with a central aspect of the natural information processing tasks that people are so good at.…We will introduce a computational framework for modeling cognitive processes that seems…closer than other frameworks to the style of computation as it might be done by the brain. The premise that PDP is using here is that traditional computer programs work very differently from brains, and that might be
==========
Deep Learning for Coders with fastai and P - Sylvain Gugger (Sylvain Gugger)
- Your Highlight on Location 363-367 | Added on Friday, December 15, 2023 7:01:45 PM

the most pivotal work in neural networks in the last 50 years was the multi-volume Parallel Distributed Processing (PDP) by David Rumelhart, James McClelland, and the PDP Research Group, released in 1986 by MIT Press.
==========
Deep Learning for Coders with fastai and P - Sylvain Gugger (Sylvain Gugger)
- Your Highlight on Location 903-904 | Added on Friday, December 15, 2023 7:19:18 PM

if you regard a neural network as a mathematical function, it turns out to be a function that is extremely flexible depending on its weights
==========
Deep Learning for Coders with fastai and P - Sylvain Gugger (Sylvain Gugger)
- Your Highlight on Location 919-922 | Added on Friday, December 15, 2023 7:21:55 PM

a neural network is a particular kind of machine learning model, which fits right in to Samuel’s original conception. Neural networks are special because they are highly flexible, which means they can solve an unusually wide range of problems just by finding the right weights. This is powerful, because stochastic gradient descent provides us a way to find those weight values automatically.
==========
Deep Learning for Coders with fastai and P - Sylvain Gugger (Sylvain Gugger)
- Your Highlight on Location 1147-1150 | Added on Monday, December 18, 2023 10:04:11 PM

Overfitting is the single most important and challenging issue when training for all machine learning practitioners
==========
Deep Learning for Coders with fastai and P - Sylvain Gugger (Sylvain Gugger)
- Your Highlight on Location 1147-1150 | Added on Monday, December 18, 2023 10:04:19 PM

Overfitting is the single most important and challenging issue when training for all machine learning practitioners, and all algorithms.
==========
Deep Learning for Coders with fastai and P - Sylvain Gugger (Sylvain Gugger)
- Your Highlight on Location 1160-1168 | Added on Monday, December 18, 2023 10:05:26 PM

Validation Set When you train a model, you must always have both a training set and a validation set, and you must measure the accuracy of your model only on the validation set. If you train for too long, with not enough data, you will see the accuracy of your model start to get worse; this is called overfitting.
==========
